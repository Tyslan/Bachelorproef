\chapter{Architectuur}
\label{ch:cassandra_architectuur}
% Peer to peer
% Cluster en snitches en partitioners

%Structuur van Cassandra Data Modeling and Analysis
\section{Partitionering}
Een van de belangrijkste zaken bij Cassandra is het horizontaal schalen.
Om dit te kunnen doen moet de data dynamisch gepartitioneerd worden over de nodes van een cluster.
Dit wordt bij Cassandra voor elkaar gekregen door het gebruik van partitioners.
De partitioner wordt ingesteld op cluster niveau.
Deze partitioners maken gebruik van hash functies om te bepalen op welke node de data moet geplaatst worden.
Bij consistente hashing bij Cassandra wordt de output behandeld als een "ring".
Elke node in de cluster krijgt een willekeurige waarde toegewezen en deze waarde bepaald dan de plaats in de ring \citep{lakshman2010cassandra}.


Er zijn drie soorten partitioners in Cassandra:

\begin{enumerate}
	\item \textbf{RandomPartitioner}:
	Dit was de standaard partitioner tot Cassandra 1.2.
	Via de MD5 hash van de rijsleutel probeert deze partitioner de data evenwichtig over alle nodes te verspreiden.
	Omdat de MD5 Hash functie vrij traag is, werd er een andere partitioner als standaard aangesteld.
	
	\item \textbf{Murmur3Partitioner}:
	Sinds Cassandra 1.2 is dit de standaard partitioner van Cassandra.
	Deze partitioner maakt gebruik van de MurmurHash functie.
	De hash wordt op deze manier een 64-bit hashwaarde van de rijsleutel.
	
	\item \textbf{ByteOrderedPartitioner}:
	Zoals de naam doet vermoeden wordt deze partitioner gebruikt voor geordende partitionering.
	Deze partitioner ordent de rijen volgens de bytes van de rijsleutel.
	De tokens worden berekend a.d.h.v. hexadecimale representatie van de leidende tekens van de rijsleutel.
	Op deze manier kan men, net zoals men met een cursor in een SQL onmgeving zou doen, de tabel geordend overlopen op primaire sleutel.
	
\end{enumerate}

\subsection{Problemen met de ByteOrderedPartitioner}
De ByteOrderedPartitioner blijkt een oplossing te bieden om tabellen te verkrijgen die gesorteerd zijn op de primaire sleutel.
Ondanks dit feit zijn er toch een aantal problemen met deze partitioner.

\begin{itemize}
	\item Sequentiële writes kunnen voor "hot spots" zorgen.
	Als een aantal rijen ongeveer gelijktijdig toevoegt of updatet, dan worden deze quasi zeker op dezelfde node weggeschreven.
	Dit is zeker een probleem voor tijdreeksen \citep{kan2014cassandra}.
	
	\item Met de ByteOrderedPartitioner is het zeer moeilijk om een gebalanceerde cluster te krijgen.
	De enige manier waarop dit verkregen kan worden is om dit manueel te doen.
	Als men dit echter niet doet is de kans vrij groot dat de data op slechts enkele nodes van de cluster opgeslagen word.
	Als men dan nog eens verschillende types zijn voor de primaire sleutels dan is het zo goed als onmogelijk om een gebalanceerde cluster te krijgen
	\citep{Bauer2013CaseAgainstByteOrder}.
\end{itemize}

\section{Replicatie}
Om de hoge beschikbaarheid te verkrijgen maakt Cassandra gebruik van data replicatie.
Door de replicatie wordt de kans op een "Single point of Failure" enorm ingeperkt.
Cassandra voorziet een methode om de replicatie factor blijft behouden zelfs als bepaalde nodes uitgevallen zijn.
Deze replicatie factor wordt ingesteld op keyspace niveau.
De replicatie factor bepaald hoeveel replica's er van de data worden bijgehouden.
Op basis van de partitioner en de replicatie strategie wordt  bepaalt waar de replica's opgeslagen worden.
Om deze replicatie in goede banen te leiden zijn er een aantal strategieën voorzien \citep{kan2014cassandra}.

\subsection{Replicatie strategieën}
% p63
De replicatie strategie bepaald hoe de data precies op de cluster geplaatst wordt.
Hierbij zijn twee strategieën:

\subsubsection{SimpleStrategy}
Deze SimpleStrategy wordt meestal gebruikt als er maar één datacenter aanwezig is.
Wanneer men over meerdere datacenters beschikt is het beter om naar de andere strategie te kijken.
Wanneer de standaard partitioner, Murmer3Partitioner, gebruikt wordt gaat de partitioner de locatie bepalen a.d.h.v. de hash van de primaire sleutel.
De replica's van de data geplaatst op de volgende nodes in de ring met de richting van de klok mee.
	
Op figuur \ref{fig:simple_strategy} wordt een voorbeeld gegeven waarbij de replicatie factor 3 is.
De zwarte namen zijn de originele date en de grijze de replica's.
	
\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{img/4_architectuur/SimpleStrategy}
	\caption{Illustratie SimpleStrategy \citep{strickland2014availability}}
	\label{fig:simple_strategy}
\end{figure}
	
	
\subsubsection{NetworkTopologyStrategy}
Op productie clusters wordt er vaak NetworkTopologyStragegy toegepast.
Hierbij gaat men ervan uit dat een productie cluster uit meerdere datacenters bestaat of in de toekomst meerdere datacenters zal bevatten.

Het grote voordeel van deze strategie is dat er rack awareness is en dat de snitches ingesteld kunnen worden.
Rack awareness zorgt ervoor dat de replica's op verschillende racks worden opgeslagen.
Dit is niet mogelijk bij simple strategy.
Ook kan men via deze strategy voor ieder datacenter een andere replicatie factor instellen.

\section{Snitches}
% p20
\section{Seed node}
% p20
\section{Gossip en foutdetectie}
% p21
\section{Herstelmechanisme}
% %p24